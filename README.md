# Text2GS: Text-to-3D Gaussian Splatting

An end-to-end pipeline for generating 3D Gaussian Splatting scenes from text descriptions.

## ğŸ“‹ Overview

Existing text-to-3D generation methods face several challenges:
- Single-view generation struggles to maintain multi-view consistency
- Direct optimization of NeRF/3D-GS is time-consuming and prone to local optima
- Lack of effective support for 360Â° panoramic views of indoor scenes

Text2GS uses a **progressive multi-stage generation framework** that decomposes the complex text-to-3D task into controllable sub-tasks, progressively building high-quality 3D scenes.

### Key Contributions

1. **Multi-view Consistent Generation**: Leverages MVDiffusion's correspondence-aware mechanism to explicitly model geometric constraints between views during diffusion, generating globally consistent 360Â° panoramic images

2. **Geometry-Appearance Decoupled Reconstruction**: Employs DUSt3R for calibration-free dense point cloud reconstruction, separating geometry estimation from appearance generation to improve reconstruction robustness

3. **Video Diffusion-Driven View Interpolation**: Applies ViewCrafter video diffusion model for sparse view interpolation, utilizing temporal consistency of video generation to ensure smooth transitions between interpolated views

4. **End-to-End Automated Pipeline**: Designs a modular four-stage pipeline supporting intermediate result visualization and parameter tuning for analysis and improvement

### Pipeline

```
Text â”€â”€â†’ MVDiffusion â”€â”€â†’ DUSt3R â”€â”€â†’ ViewCrafter â”€â”€â†’ 3D-GS
         (Multi-view)    (Point Cloud) (Interpolation) (Scene)
```

| Stage | Input | Output | Key Techniques |
|-------|-------|--------|----------------|
| Stage 1 | Text prompt | 8 panoramic images | Correspondence-aware diffusion |
| Stage 2 | Multi-view images | Point cloud + poses | Calibration-free 3D reconstruction |
| Stage 3 | Sparse views + point cloud | Dense view sequence | Video diffusion, point cloud rendering |
| Stage 4 | Dense views + poses | 3D-GS scene | COLMAP format export |

## ğŸ› ï¸ Requirements

- Python 3.10
- CUDA 12.8
- GPU: 24GB+ VRAM recommended (e.g., RTX 4090, A100)
- Disk space: ~20GB (model weights)

## ğŸ“¦ Installation

### 1. Clone the repository

```bash
git clone https://github.com/Flowow-zjw/Text2GS
cd Text2GS
```

### 2. Create virtual environment

```bash
conda create -n text2gs python=3.10
conda activate text2gs
```

### 3. Install PyTorch (CUDA 12.8)

```bash
pip install torch==2.7.1+cu128 torchvision==0.22.1+cu128 torchaudio==2.7.1+cu128 --index-url https://download.pytorch.org/whl/cu128
```

### 4. Install dependencies

```bash
pip install -r requirements.txt
```

### 5. Install PyTorch3D

```bash
# Option 1: Direct installation (recommended)
pip install "git+https://github.com/facebookresearch/pytorch3d.git"

# Option 2: Manual build
git clone https://github.com/facebookresearch/pytorch3d.git
cd pytorch3d
pip install -e .
cd ..
```

### 6. Clone external dependencies

```bash
# Create extern directory
mkdir -p extern

# MVDiffusion
git clone https://github.com/Tangshitao/MVDiffusion.git ./extern/MVDiffusion

# ViewCrafter
git clone https://github.com/Drexubery/ViewCrafter.git ./extern/ViewCrafter

# DUSt3R
git clone --recursive https://github.com/naver/dust3r.git ./extern/dust3r
```

### 7. Download model weights

Create checkpoint directories:

```bash
mkdir -p checkpoints/mvdiffusion
mkdir -p checkpoints/viewcrafter
mkdir -p checkpoints/dust3r
```

Download the following models:

| Model | Download Link | Save Path |
|-------|---------------|-----------|
| MVDiffusion Panorama | [Dropbox](https://www.dropbox.com/scl/fi/yx9e0lj4fwtm9xh2wlhhg/pano.ckpt?rlkey=kowqygw7vt64r3maijk8klfl0&dl=0) | `checkpoints/mvdiffusion/pano.ckpt` |
| ViewCrafter Sparse | [HuggingFace](https://huggingface.co/Drexubery/ViewCrafter_25_sparse/resolve/main/model_sparse.ckpt) | `checkpoints/viewcrafter/model_sparse.ckpt` |
| DUSt3R | [NaverLabs](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth) | `checkpoints/dust3r/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth` |

Command line download:

```bash
# MVDiffusion (manual download from Dropbox required)

# ViewCrafter
wget https://huggingface.co/Drexubery/ViewCrafter_25_sparse/resolve/main/model_sparse.ckpt -O checkpoints/viewcrafter/model_sparse.ckpt

# DUSt3R
wget https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth -O checkpoints/dust3r/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth
```

## ğŸš€ Usage

### Basic usage

```bash
python -m text2gs.run --text "A cozy living room with a fireplace and wooden furniture"
```

### Specify output directory

```bash
python -m text2gs.run --text "..." --output ./my_output
```

## ğŸ“ Project Structure

```
Text2GS/
â”œâ”€â”€ text2gs/                    # Core code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ run.py                  # CLI entry point
â”‚   â”œâ”€â”€ pipeline.py             # Pipeline logic
â”‚   â”œâ”€â”€ stages/                 # Stage implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py             # Base class
â”‚   â”‚   â”œâ”€â”€ mvdiffusion.py      # Stage 1: Multi-view generation
â”‚   â”‚   â”œâ”€â”€ pointcloud.py       # Stage 2: Point cloud reconstruction
â”‚   â”‚   â”œâ”€â”€ viewcrafter.py      # Stage 3: Dense view synthesis
â”‚   â”‚   â””â”€â”€ gaussian.py         # Stage 4: 3D-GS export
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ camera.py           # Camera utilities
â”‚       â”œâ”€â”€ render.py           # Rendering tools
â”‚       â””â”€â”€ io.py               # File I/O
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â””â”€â”€ default.yaml
â”œâ”€â”€ extern/                     # External dependencies (clone manually)
â”‚   â”œâ”€â”€ MVDiffusion/
â”‚   â”œâ”€â”€ ViewCrafter/
â”‚   â””â”€â”€ dust3r/
â”œâ”€â”€ checkpoints/                # Model weights (download manually)
â”‚   â”œâ”€â”€ mvdiffusion/
â”‚   â”‚   â””â”€â”€ pano.ckpt
â”‚   â”œâ”€â”€ viewcrafter/
â”‚   â”‚   â””â”€â”€ model_sparse.ckpt
â”‚   â””â”€â”€ dust3r/
â”‚       â””â”€â”€ DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth
â”œâ”€â”€ output/                     # Output directory
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“¤ Output Structure

Results are saved to `output/<timestamp>/`:

```
output/20260101_120000/
â”œâ”€â”€ stage1_mvdiffusion/         # Stage 1 output
â”‚   â”œâ”€â”€ view_00.png ~ view_07.png   # 8 panoramic views
â”‚   â”œâ”€â”€ cameras.npz             # Camera parameters
â”‚   â”œâ”€â”€ prompt.txt              # Input prompt
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ stage2_pointcloud/          # Stage 2 output
â”‚   â”œâ”€â”€ pointcloud.ply          # Sparse point cloud
â”‚   â”œâ”€â”€ images/                 # Input images
â”‚   â”œâ”€â”€ depths/                 # Depth maps
â”‚   â”œâ”€â”€ cameras.npz             # Optimized camera parameters
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ stage3_viewcrafter/         # Stage 3 output
â”‚   â”œâ”€â”€ videos/                 # Generated videos
â”‚   â”œâ”€â”€ frames/                 # All frame images
â”‚   â”œâ”€â”€ pointcloud.ply          # Updated point cloud
â”‚   â”œâ”€â”€ cameras.npz             # Interpolated camera parameters
â”‚   â””â”€â”€ metadata.json
â””â”€â”€ 3dgs/                       # Stage 4 output (COLMAP format)
    â”œâ”€â”€ images/                 # Training images
    â”œâ”€â”€ sparse/0/               # COLMAP sparse reconstruction
    â”‚   â”œâ”€â”€ cameras.bin
    â”‚   â”œâ”€â”€ images.bin
    â”‚   â””â”€â”€ points3D.bin
    â””â”€â”€ colmap_output.txt
```

## ğŸ“š Citation

```bibtex
@inproceedings{tang2023mvdiffusion,
  title={MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion},
  author={Tang, Shitao and Zhang, Fuyang and Chen, Jiacheng and Wang, Peng and Furukawa, Yasutaka},
  booktitle={NeurIPS},
  year={2023}
}

@article{yu2024viewcrafter,
  title={ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis},
  author={Yu, Wangbo and Xing, Jinbo and Yuan, Li and Hu, Wenbo and Li, Xiaoyu and others},
  journal={TPAMI},
  year={2025}
}

@inproceedings{wang2024dust3r,
  title={DUSt3R: Geometric 3D Vision Made Easy},
  author={Wang, Shuzhe and Leroy, Vincent and Cabon, Yohann and Chidlovskii, Boris and Revaud, Jerome},
  booktitle={CVPR},
  year={2024}
}

@article{kerbl3Dgaussians,
  title={3D Gaussian Splatting for Real-Time Radiance Field Rendering},
  author={Kerbl, Bernhard and Kopanas, Georgios and Leimk{\"u}hler, Thomas and Drettakis, George},
  journal={ACM TOG},
  year={2023}
}
```

## ğŸ“„ License

MIT License
